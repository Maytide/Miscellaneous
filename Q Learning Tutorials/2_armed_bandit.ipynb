{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_armed_bandit.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "W1W01If3IL6X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jIE5n1LeIlcJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#List out our bandits. Currently bandit 4 (index#3) is set to most often provide a positive reward.\n",
        "\n",
        "\n",
        "# doesn't work with negative values smaller than around this a and b.\n",
        "a = -2.5\n",
        "b = -3.95\n",
        "c = 17\n",
        "d = 0.001\n",
        "bandits = [[a, b, c, d],[b, c, d, a],[c, d, a, b],[d, a, b, c]]\n",
        "num_bandits = len(bandits)\n",
        "cycle = 0\n",
        "def pullBandit(action):\n",
        "    global cycle\n",
        "    #Get a random number.\n",
        "    result = np.random.randn(1)\n",
        "    if result > bandits[cycle & 3][action]:\n",
        "        #return a positive reward.\n",
        "        k = 1\n",
        "        if (cycle & 3) >= 1: k /= cycle\n",
        "    else:\n",
        "        #return a negative reward.\n",
        "        k = -1\n",
        "        if (cycle & 3) >= 1: k /= cycle\n",
        "    \n",
        "    cycle += 1\n",
        "    return k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcMBB8iwInaF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "#These two lines established the feed-forward part of the network. This does the actual choosing.\n",
        "weights = tf.Variable(tf.ones([num_bandits]))\n",
        "chosen_action = tf.argmax(weights,0)\n",
        "\n",
        "#The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
        "#to compute the loss, and use it to update the network.\n",
        "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
        "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
        "responsible_weight = tf.slice(weights,action_holder,[1])\n",
        "loss = -(tf.log(responsible_weight)*reward_holder)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "update = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpWczpUNIvvG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "outputId": "b2c520b6-9cc2-4c9c-bc00-04de14795fb2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525106487552,
          "user_tz": 240,
          "elapsed": 32574,
          "user": {
            "displayName": "George Zhang",
            "photoUrl": "//lh6.googleusercontent.com/-9PqbapyYYJU/AAAAAAAAAAI/AAAAAAAACcI/M5sw0u4rr-g/s50-c-k-no/photo.jpg",
            "userId": "116297170449001352312"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "total_episodes = 50000 #Set total number of episodes to train agent on.\n",
        "total_reward = np.zeros(num_bandits) #Set scoreboard for bandits to 0.\n",
        "e = 0.1 #Set the chance of taking a random action.\n",
        "\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Launch the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    while i < total_episodes:\n",
        "        \n",
        "        #Choose either a random action or one from our network.\n",
        "        if np.random.rand(1) < e:\n",
        "            action = np.random.randint(num_bandits)\n",
        "        else:\n",
        "            action = sess.run(chosen_action)\n",
        "        \n",
        "        reward = pullBandit(action) #Get our reward from picking one of the bandits.\n",
        "        \n",
        "        #Update the network.\n",
        "        _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
        "        \n",
        "        #Update our running tally of scores.\n",
        "        total_reward[action] += reward\n",
        "        if i % 1000 == 0:\n",
        "            print(\"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n",
        "        i+=1\n",
        "print(\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
        "if np.argmax(ww) == np.argmax(-np.array(bandits[0])):\n",
        "    print(\"...and it was right!\")\n",
        "else:\n",
        "    print(\"...and it was wrong!\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running reward for the 4 bandits: [0. 1. 0. 0.]\n",
            "Running reward for the 4 bandits: [  9.99233922 226.46702874  -5.99378022   1.98235498]\n",
            "Running reward for the 4 bandits: [ 14.98992898 460.4744286  -10.98123336   5.99453335]\n",
            "Running reward for the 4 bandits: [ 22.98947222 687.49068704 -16.9785675    4.99365627]\n",
            "Running reward for the 4 bandits: [ 27.98783854 921.49593733 -19.97492744   8.99531784]\n",
            "Running reward for the 4 bandits: [  32.9884064  1151.49731196  -25.9721688    13.99683899]\n",
            "Running reward for the 4 bandits: [  35.98778698 1385.49195502  -31.9690362    14.99845981]\n",
            "Running reward for the 4 bandits: [  39.98714588 1621.48957191  -34.96685607   16.00016556]\n",
            "Running reward for the 4 bandits: [  46.98739435 1850.4858404   -40.9653958    16.00178585]\n",
            "Running reward for the 4 bandits: [  52.98760953 2083.48415927  -46.96456754   17.00236886]\n",
            "Running reward for the 4 bandits: [  60.98709932 2308.48470555  -58.96371601   14.00321324]\n",
            "Running reward for the 4 bandits: [  66.98767685 2541.48366534  -64.96237359   17.00369054]\n",
            "Running reward for the 4 bandits: [  69.98759063 2780.48420052  -69.96115305   16.00395576]\n",
            "Running reward for the 4 bandits: [  77.98750496 3007.48152255  -76.95964118   12.00467369]\n",
            "Running reward for the 4 bandits: [  81.98736632 3235.48417047  -85.95874009   13.00511301]\n",
            "Running reward for the 4 bandits: [  88.98771343 3468.4846947   -90.95811922   12.00545843]\n",
            "Running reward for the 4 bandits: [  92.9872618  3702.48527881  -95.95734658   15.0058474 ]\n",
            "Running reward for the 4 bandits: [  94.98732214 3939.48446116 -102.95692293   15.00638747]\n",
            "Running reward for the 4 bandits: [ 102.98715155 4169.48309443 -110.95629653   15.00661112]\n",
            "Running reward for the 4 bandits: [ 105.98741868 4406.4830133  -115.95585597   16.00704127]\n",
            "Running reward for the 4 bandits: [ 108.98711358 4641.48140367 -121.95523962   16.00719609]\n",
            "Running reward for the 4 bandits: [ 112.98740665 4876.48144369 -125.95499763   13.00759082]\n",
            "Running reward for the 4 bandits: [ 114.98735924 5117.48117423 -127.95457798   14.00777296]\n",
            "Running reward for the 4 bandits: [ 119.98731938 5344.48170037 -138.95404064   17.00817036]\n",
            "Running reward for the 4 bandits: [ 125.98757942 5569.48180532 -146.95357332   14.00838094]\n",
            "Running reward for the 4 bandits: [ 136.98761711 5798.48230151 -152.95284091   14.00842196]\n",
            "Running reward for the 4 bandits: [ 138.98785173 6027.481751   -160.95233391   15.00869353]\n",
            "Running reward for the 4 bandits: [ 142.9878146  6262.48130735 -165.9518819    17.00865836]\n",
            "Running reward for the 4 bandits: [ 150.98796092 6495.48146034 -167.95145033   20.00873125]\n",
            "Running reward for the 4 bandits: [ 158.9876774  6726.48134833 -173.95102789   19.00897568]\n",
            "Running reward for the 4 bandits: [ 163.98794797 6962.48202053 -176.95072113   21.00907495]\n",
            "Running reward for the 4 bandits: [ 169.98791386 7193.48165807 -186.95036139   20.00930253]\n",
            "Running reward for the 4 bandits: [ 176.98797631 7427.48207029 -192.9501707    23.00952574]\n",
            "Running reward for the 4 bandits: [ 181.98788458 7661.48182435 -197.95007813   21.00977138]\n",
            "Running reward for the 4 bandits: [ 186.98782459 7891.48066329 -207.94965895   24.01003873]\n",
            "Running reward for the 4 bandits: [ 192.987825   8119.48069244 -216.94934023   25.01038718]\n",
            "Running reward for the 4 bandits: [ 198.987572   8354.48023574 -219.94900266   25.01058493]\n",
            "Running reward for the 4 bandits: [ 200.9875179  8586.48024    -225.94870308   25.01088821]\n",
            "Running reward for the 4 bandits: [ 202.98773287 8820.47995046 -234.94838483   26.01123534]\n",
            "Running reward for the 4 bandits: [ 210.98770628 9054.4801369  -237.94794317   27.01146985]\n",
            "Running reward for the 4 bandits: [ 216.98763045 9281.48031193 -249.94756484   28.0116969 ]\n",
            "Running reward for the 4 bandits: [ 221.98772877 9513.480333   -257.94711984   27.0118197 ]\n",
            "Running reward for the 4 bandits: [ 223.98770388 9743.48085927 -270.94671015   26.01196444]\n",
            "Running reward for the 4 bandits: [ 231.98763357 9974.48099793 -274.94623885   21.0121286 ]\n",
            "Running reward for the 4 bandits: [  236.98749668 10206.48088488  -281.94570992    17.01217424]\n",
            "Running reward for the 4 bandits: [  240.98760871 10441.4813321   -287.94534959    14.01224082]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running reward for the 4 bandits: [  250.98760864 10664.48034032  -299.94495509    15.01230741]\n",
            "Running reward for the 4 bandits: [  255.98760936 10901.48068987  -304.9446755     16.01241528]\n",
            "Running reward for the 4 bandits: [  268.98758879 11125.48064538  -314.94442356    15.01239436]\n",
            "Running reward for the 4 bandits: [  275.98763049 11355.48029527  -322.94423885    16.01264165]\n",
            "The agent thinks bandit 2 is the most promising....\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s5eKHHUAI_ul",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}